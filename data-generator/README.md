# Synthetic Data Generation

This open-source package includes a synthetic data generator and a [companion app](https://huggingface.co/spaces/Intel/synthetic-data-generator) that use a language model to create data based on specified categories and labels. By using this package, you can generate your own synthetic data for various use cases, such as training and testing machine learning models. The synthetic data generator can be run on [Intel® Tiber™ AI Cloud](https://cloud.intel.com/) using an Intel® Xeon® CPU instance. This platform provides ample computing resources ensuring smooth execution of your code.

## Table of Contents

- [Article](#article)
- [Preparation to Run on Intel Tiber AI Cloud](#preparation-to-run-on-intel-tiber-ai-cloud)
- [Installation](#installation)
- [Usage](#usage)
- [Configuration](#configuration)
- [Functions](#functions)
- [Join the Community](#join-the-community)
- [License](#license)

## Article

Visit [Synthetic Data Generation with Language Models: A Practical Guide](https://medium.com/p/0ff98eb226a1) to learn about the implementation of this package. For more AI development how-to content, visit [Intel® AI Development Resources](https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/overview.html).

## Preparation to Run on Intel Tiber AI Cloud

1. Visit <https://cloud.intel.com/> and sign up.

2. Go to the "Learning" tab and click "Connect now" to launch JupyterLab*.

## Installation

1. Clone the repository:
    ```sh
    git clone https://github.com/intel/polite-guard.git
    ```

2. Install the `uv` package and project manager either by following the [OS-specific instructions](https://docs.astral.sh/uv/getting-started/installation/#standalone-installer). On Intel Tiber AI Cloud, you can run

    ```sh
    curl -LsSf https://astral.sh/uv/install.sh | sh
    ```
    in a terminal to install uv.

## Usage

To run the synthetic data generator, navigate to `polite-guard/data-generator` and use a variation of the following command:

```sh
uv run sdg.py --sample_size 100 --batch_size 20 --output_dir ./output --model meta-llama/Llama-3.2-3B-Instruct --save_reasoning
```

`uv` will automatically create a virtual environment and install the necessary packages for you.

### Arguments

- `--config`: The configuration file for the sdg function containing labels, categories, and examples (default: ./config/polite-guard-config.py).
- `--sample_size`: The number of samples generated by the language model (default: 100).
- `--model`: The language model for data generation (default: `meta-llama/Llama-3.2-3B-Instruct`).
- `--max_new_tokens`: The maximum number of new tokens to generate for each sample (default: 256).
- `--batch_size`: The batch size for saving generated samples to file (default: 20).
- `--output_dir`: The output directory (default: `./`).
- `--save_reasoning`: Enable save reasoning (default: False).

## Configuration

The configuration for labels, label descriptions, categories, and other parameters should be stored in a Python file structured as follows:

```python
labels = ["Label1", "Label2", "Label3"]
label_descriptions = "Description of labels"
categories_types = {
    "Category1": ["Type1", "Type2"],
    "Category2": ["Type3", "Type4"]
}
use_case = "Your use case"
prompt_examples = "Examples for the Few-Shot Chain-of-Thought prompt."
```
You may find examples of such config files under the `config` directory.

## Functions

- `read_token() -> None`

Logs into Hugging Face using a token stored in a '.env' file under the key `HF_TOKEN`. (See [Article](#article) to learn how to create and access your Hugging Face token.)
If the '.env' file is missing or `HF_TOKEN` is not provided, the user will be prompted to log in.

- `validate_positive_integer(value: str) -> int`

Validates that the input string, provided via command-line arguments, represents a positive integer.

- `parse_string(input_string: str) -> Tuple[str, str]`

Parses a string containing `OUTPUT:` and `REASONING:` sections and extracts their values.

- `sdg(sample_size: int, labels: List[str], label_descriptions: str, categories_types: Dict[str, str], use_case: str, prompt_examples: str, batch_size: int, output_dir: str, model: str, save_reasoning: bool) -> None`

Generates synthetic data based on specified categories and labels.

- `main()`
The main function that parses command-line arguments and runs the synthetic data generator.

## Join the Community

If you are interested in exploring other models, join us in the Intel and Hugging Face communities. These models simplify the development and adoption of Generative AI solutions, while fostering innovation among developers worldwide. Here are some ways you can contribute:

### 1. Star and Share
If you find this project valuable, please give it a ⭐ on GitHub and share it with your network. Your support helps us grow the community and reach more contributors.

### 2. Contribute Code or Documentation
Help us improve and expand the project by contributing:
- **Code**: Fix bugs, optimize performance, or add new features.
- **Documentation**: Enhance the documentation to make it more accessible and user-friendly.

Check out the [Contributing Guide](../CONTRIBUTING.md) to get started.

### 3. Test and Provide Feedback
Run the software on your Intel hardware and share your experience. Report issues, suggest improvements, or request new features through the issues tab on GitHub.

### 4. Extend and Integrate
Use this project as a foundation for your own work. Build new applications or integrate it with other tools and libraries. Let us know what you create--we'd love to feature your work!

### 5. Spread the Word
Help us amplify our message by blogging, tweeting, or presenting about the project at conferences or meetups. Tag us and use our official hashtag so we can share your content with the community.

## License

This project is licensed under the [MIT License](../LICENSE).

*Other names and brands may be claimed as the property of others.
